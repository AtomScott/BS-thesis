Subspace analysis is a term used to describe a general framework used in computer vision, that is used for the comparison and classification of subspaces. In this section, we introduce typical approaches in subspace analysis, in particular the classification of subspaces, and describe how it can be extended to analyse shapes such as human poses.

\subsection{Shape Subspace}

Given a matrix $X \in \mathbb{R}^{n \times 3}$ where each row represents a three-dimensional coordinate of some shape, in this case a point on the human body, we can generate a shape matrix by translating the coordinate system so the each coordinate is relative to center, i.e the root, of the shape\cite{Igarashi20113DSubspaces}.  

The columns that a shape matrix spans is called a shape subspace. A shape subspace is invariant under affine projection. It is also invariant to changes of coordinates caused by camera rotation and object motions. This characteristic of the shape subspace has made it effective in various tasks, such as motion segmentation and sequential factorization. Since shape subspace is invariant to affine projection, a subspace similarity metric, such as canonical angles, can effectively represent the geometrical relation between two shape subspaces. 

The following methods (subspace method, mutual space method, Grassmann Discriminant Analysis etc.) are used to classify subspaces. Thus they can be applied to shape subspaces without any modification.

\newpage
%*================================================================

\subsection{Subspace Method}

The Subspace method assumes an input vector $\mathbf{x}$ and $k$-class subspaces. Each class subspace approximates a data distribution for a single class. This approximation is obtained by applying Principle Component Analysis (PCA) to each class. The similarity $S$ of the input vector $\mathbf{x}$ to the $i^{th}$ class subspace $\pazocal{P}$ is defined based on either:

\begin{itemize}
    \item The length of the  projection of $\mathbf{x}$ to $\pazocal{P}$ \cite{WATANABE1967EvaluationRecognition}.
    \item The minimum angle between $\mathbf{x}$ and $\pazocal{P}$ \cite{Iijima1973THEORYMETHOD.}.
\end{itemize}

The length of an input vector $\mathbf{x}$ is often normalized to 1. In this case these two criteria are identical. Since they are the same, from here on we think of the angle-based similarity $S$ defined by the following equation:

\[S = \cos^2{\theta} = \sum^{k}_{i=1} \frac{(\mathbf{x}^\top \cdot \Phi_i)^2}{||\mathbf{x2}||^2}\]

$\Phi_i$ is the $i^{th}$ orthogonal normal basis vector of the class subspace $\pazocal{P}$, which are obtained from applying the PCA to a set of patterns of the class. In more rigorous terms, these orthonormal basis vectors can be obtained as the eigenvectors of the correlation matrix

\[\sum^{l}_{i=1}\mathbf{x}^{(i)}\mathbf{x}^{(i)\top}  \ \ where \\ \ \mathbf{x}^{(i)} \in \mathbb{X} \]

\noindent of the class ($\mathbb{X}$ is the training dataset).

{\setlength{\parindent}{0cm} \vspace{5mm}

\emph{Learning Phase}

\begin{enumerate}
    \item Generate $k$ class subspaces from each class by using PCA.
\end{enumerate}

\emph{Recognition Phase}

\begin{enumerate}
    \item Calculate $S$ between $\mathbf{x}$ and each subspace $\pazocal{P}$, $\pazocal{Q}$ etc.
    \item Classify the $\mathbf{x}$ into the class of the subspace where $S$ was calculated to be the highest.
\end{enumerate}
}
\newpage
%*================================================================

\subsection{Mutual Subspace Method}

The Mutual Subspace Method (MSM) is an extension of the Subspace Method (SM), where instead of having an input vector $\mathbf{x}$, we use an input subspace $\pazocal{P}$. MSM is commonly used for image set classification \cite{sakai2019gait}.

The Subspace method assumes an input subspace and $k$ class subspaces. Let us define the input subspace to be a $d_p$-dimensional subspace $\pazocal{P}$ and the class subspaces to be $d_q$-dimensional subspaces $\{\pazocal{P}, \pazocal{Q}, \pazocal{R}...\}$.

The similarity $S$ between, for example, $\pazocal{P}$ and $\pazocal{Q}$ was originally defined as the minimum canonical angle $\theta_1$. Canonical angles \cite{chatelin2012} are uniquely defined as:

\[    \cos^2 \theta_i = \max_{\substack{u_i \perp u_j(=1,...,i−1) \\ v_i \perp v_j(=1,...,i−1)}} \frac{|(u_1,v_i)|^2}{||u_i||^2||v_i||^2}\]

Where $\mathbf{u}_i \in \pazocal{P}$, $\mathbf{v}_i \in \pazocal{Q}$, $||\mathbf{u}_i|| \neq 0$, $||\mathbf{v}_i|| \neq 0$.

We can also include the remaining canonical angles when calculating the similarity.


\[    \tilde{S} = \frac{1}{t} \sum^{t}_{i=1}\cos{\theta_i}^2 \]

This value $\tilde{S}$ reflects the structural similarity between two subspaces. It is also defined on the $t$ smallest canonical angles. For practical applications, the canonical angles between subspaces $\pazocal{P}_1$ and $\pazocal{Q}$ are obtained by calculating the singular values $\{\lambda_j\}_{j=1}^{m}$ of the correlation matrix between their basis matrices $\mathbf{P}, \mathbf{Q} \in \mathbb{R}^{d{\times}m}$, i.e. solving the SVD of $\mathbf{V}\mathbf{\Lambda}\mathbf{V}^{\top} = \mathbf{P}^{\top}\mathbf{Q}$. This corresponds to finding the rotation of each basis that is closest to the opposing subspace. From solving this problem, the canonical angles can be obtained by $\theta_j = \cos^{-1}(\lambda_j), \text{ where } j = 1, \ldots, m$.

{\setlength{\parindent}{0cm} \vspace{5mm}

\emph{Learning Phase}

\begin{enumerate}
    \item Generate $k$ class subspaces from each class by using PCA.
\end{enumerate}

\emph{Recognition Phase}

\begin{enumerate}
    \item Calculate $\tilde{S}$ between the input subspace and each dictionary subspace.
    \item Classify the input subspace into the class where the dictionary subspace was calculated to be the highest.
\end{enumerate}
}
\newpage
%*================================================================

\subsection{Grassmann Discriminant Analysis}
Next, we introduce here the concept Grassman Discriminant Analysis(GDA)\cite{Hamm2008GrassmannLearning} which is a discriminatory mechanism to separate classes of signals. First we will, briefly explain its predecessor, Linear Discriminant Analysis (LDA)~\cite{fukunaga1990statistical}. LDA is a dimensionality reduction technique that is is well known and has been successfully used for classification. In essence, GDA is conducted as kernel LDA but with the Grassmann kernels.

\subsubsection{Linear Discriminant Analysis}
The goal of Linear Discriminant Analysis (LDA) is to project a dataset onto a lower-dimensional space that has good class-separability. Due to the curse of dimensionality, dimensionality reduction is an simport step that is necessary in order avoid overfitting. It also has the added benefit of reducing computational costs.

{\setlength{\parindent}{0cm} \vspace{5mm}

\emph{Learning Phase}

\begin{enumerate}
    \item Compute $d$-dimensional mean vectors for the different classes from the dataset.
    \item Compute the inbetween-class and within-class scatter matrices.
    \item Computer the eigenvectors and corresponding eigenvalues for the scatter matrices.
    \item Sort the eigenvectors by decreasing eigenvalues and choose k eigenvectors with the largest eigenvalues to form a d×k dimensional matrix W
\end{enumerate}

\emph{Recognition Phase}

\begin{enumerate}
        \item Project the samples onto the new subsapce, YY=XX×WW (where XX is a n×d-dimensional matrix representing the n samples, and yy are the transformed n×k-dimensional samples in the new subspace).
        \item Conduct classification using a classifier of choice (a simple K-Neighbors classifier is commonly used).
\end{enumerate}


\subsubsection{Grassmann Manifold and Grassmann Kernel}

The Grassmann manifold $\pazocal{G}(m, d)$ is defined as the set of $m$-dimensional linear subspaces of $\mathbb{R}^{d}$. 

%It is an $m(d-m)$-dimensional compact Riemannian manifold anid can be derived as a quotient space of orthogonal groups $\pazocal{G}(m, d) = \pazocal{O}(d)/\pazocal{O}(m) \times \pazocal{O}(d - m)$, where $\pazocal{O}(m)$ is the group of $m \times m$ orthonormal matrices.

A Grassmann manifold can be embedded in a reproducing kernel Hilbert space by the use of a Grassmann kernel. In this case, the most popular kernel is the projection kernel $k_{p}$, which can be defined as $k_{p}(\pazocal{Y}_{1},\pazocal{Y}_{2}) = \sum_{j=1}^{m} \cos^{2} \theta_{j}$, which is homologous to the subspace similarity.
%%\begin{equation}
%%% d_{p}(\pazocal{Y}_{1},\pazocal{Y}_{2}) = \sqrt[]{\sum_{i=1}^{m} \sin^{2} \%%%theta_{i} }
%%k_{p}(\pazocal{Y}_{1},\pazocal{Y}_{2}) = \sum_{i=1}^{m} \cos^{2} \theta_{i}.
%%\end{equation}
We can measure the distance between two points on a Grassmann manifold by using this projection kernel~\cite{hamm2008thesis}, and a subspace $\pazocal{Y}$ can be represented by a vector with regards to a reference subspace dictionary $\{\pazocal{Y}_{q}\}_{q=1}^{N}$ as $\mathbf{y} = k_{p}(\pazocal{Y},\pazocal{Y}_{q}) = [k_{p}(\pazocal{Y},\pazocal{Y}_{1}),k_{p}(\pazocal{Y},\pazocal{Y}_{2}),\ldots,k_{p}(\pazocal{Y},\pazocal{Y}_{N})] \in \mathbb{R}^{N}$.

% \subsection{Algorithm of Grassmann Discriminant Analysis}
% \label{sec:gda}

%Its predecessor, linear discriminant analysis (LDA)~\cite{fukunaga1990statistical}, followed by a K-NN classifier, is well known and has been successfully used for classification.
% We first outline the algorithm of linear discriminant analysis (LDA)~\cite{fukunaga1990statistical}.
% Let ${\mathbf{x}_{1}, \ldots, \mathbf{x}_{N}}$ be the data vectors and ${y_{1}, \ldots, y_{N}}$ $(y_{i} \in {1, \ldots, C})$ be the class labels.
% Each class $c$ has $N_{c}$ number of samples.
% Let $\mathbf{\mu}_{c} = \frac{1}{N_{c}}\sum_{i|y_{i} = c} \mathbf{x}_{i}$ be the mean of class $c$, and $\mathbf{\mu} = \frac{1}{N} \sum_{i} \mathbf{x}_{i}$ be the overall mean. LDA searches for the discriminant direction $\mathbf{w}$ which maximizes the Rayleigh quotient
% $Ra(\mathbf{w}) = \mathbf{w}'\mathbf{S}_{b}\mathbf{w}/\mathbf{w}'\mathbf{S}_{w}\mathbf{w}$ where $\mathbf{S}_{b}$ and $\mathbf{S}_{w}$ are the between-class and within-class covariance matrices respectively:

% \begin{align}
%     \mathbf{S}_{b} = & \frac{1}{N} \sum_{c=1}^{C} N_{c}(\mathbf{\mu}_{c} - \mathbf{\mu})(\mathbf{\mu}_{c} - \mathbf{\mu})^{\top},                   \\
%     \mathbf{S}_{w} = & \frac{1}{N} \sum_{c=1}^{C} \sum_{i|y_{i} = c} (\mathbf{x}_{i} - \mathbf{\mu}_{c})(\mathbf{x}_{i} - \mathbf{\mu}_{c})^{\top}.
% \end{align}

% The optimal $\mathbf{w}$ is obtained from the largest eigenvector
% of $\mathbf{S}_{w}^{-1}\mathbf{S}_{b}$. Since $\mathbf{S}_{w}^{-1}\mathbf{S}_{b}$ has rank $C - 1$, there are $C - 1$ optima $\mathbf{W} = [ \mathbf{w}_{1}, \ldots, \mathbf{w}_{C-1} ]$.
% By projecting data onto the space spanned by $\mathbf{W}$, we
% achieve dimensionality reduction and feature extraction
% of data onto the most discriminant subspace.

% Kernel LDA~\cite{scholkopft1999fisher,baudat2000generalized,li2001constructing} can be formulated by using the kernel trick as follows. Let $\Gamma: \mathbb{R}^ {d} \rightarrow \pazocal{F}$ be a non-linear map from the input space $\mathbb{R}^ {d}$ to a feature space $\pazocal{F}$,
% and $\Gamma = [\mathbf{\gamma}_{1},\ldots,\mathbf{\gamma}_{N}]$ be the feature matrix of the mapped training points $\mathbf{\gamma}_{i}$. Assuming $\mathbf{w}$ is a linear combination of those feature vectors, $\mathbf{w} = \mathbf{\Gamma}\mathbf{\alpha}$, we can use the kernel trick and rewrite the Rayleigh quotient in terms of $\mathbf{\alpha}$ as:

\begin{eqnarray}
    Ra(\mathbf{\alpha}) &=& \frac{\mathbf{\alpha}^{\top}\mathbf{\Gamma}^{\top}\mathbf{S}_{b}\mathbf{\Gamma} \mathbf{\alpha}}{\mathbf{\alpha}^{\top}\mathbf{\Gamma} ^{\top}\mathbf{S}_{w}\mathbf{\Gamma} \mathbf{\alpha}} = \nonumber\\
    &=& \frac{\mathbf{\alpha}^{\top}\mathbf{K}(\mathbf{V} - \mathbf{e}_{N}\mathbf{e}_{N}^{\top}/N)\mathbf{K}\mathbf{\alpha}}{\mathbf{\alpha}^{\top}(\mathbf{K}(\mathbf{I}_{N}-\mathbf{V})\mathbf{K}+ \sigma^{2}\mathbf{I}_{N})\mathbf{\alpha}} = \nonumber\\
    &=& \frac{\mathbf{\alpha}^{\top}\mathbf{\Sigma}_{b}\mathbf{\alpha}}{\mathbf{\alpha}^{\top}(\mathbf{\Sigma}_{w}+\sigma^{2}\mathbf{I}_{N})\mathbf{\alpha}},
\end{eqnarray}

%\vfill\pagebreak

\noindent where $\mathbf{K}$ is the kernel matrix, $\mathbf{e}_{N}$ is a vector
of ones that has length $N$, $\mathbf{V}$ is a block-diagonal matrix
whose $c$-th block is the matrix $\mathbf{e}_{N_{c}}\mathbf{e}_{N_{c}}^{\top}/ N_{c}$, and $\mathbf{\Sigma}_{b}~=~\mathbf{K}(\mathbf{V} - \mathbf{e}_{N}\mathbf{e}_{N}^{\top}/N)\mathbf{K}$.
For example, the kernel matrix, $\mathbf{K}$, is calculated as the similarity matrix between subspaces $\mathbf{Y}_{q}$ and $\mathbf{Y}_{w}$.
The term $\sigma^{2} \mathbf{I}_{N}$ is used for regularizing the covariance matrix $\mathbf{\Sigma}_{w} = \mathbf{K}(\mathbf{I}_{N}  -\mathbf{V})\mathbf{K}$. It is composed of the covariance shrinkage factor $\sigma^{2} > 0$, and the identity matrix $\mathbf{I}_{N}$ of size $N$. The set of optimal vectors $\mathbf{\alpha}$ are computed from the eigenvectors of $(\mathbf{\Sigma}_{w}+\sigma^{2}\mathbf{I}_{N})^{-1}\mathbf{\Sigma}_{b}$.

We apply the GDA algorithm to the reference subspaces $\mathbf{Y}_{i}^{c}$ to generate reference vectors $\mathbf{y}_{i}^{c}$. When given an unknown bioacoustic signal $\mathbf{x}_{in}(t)$, we compute its SSA subspace $\pazocal{Y}_{in}$ and map it onto the manifold to generate a vector $\mathbf{y}_{in}$; then we predict its corresponding bioacoustic class (e.g. species)  based on the nearest reference vector (1-NN).

% \section{Time Series Classification}
% This section includes a brief introduction to methods which are commonly used in time series classification(TSC). A general pipeline in TSC is comprised of x steps. These are 1. Pre-processing, 2. Feature Extraction, 3. Classification. In some cases such as deep learning, feature extraction and classification are performed simultaneously. 

% \subsection{Pre-processing for TSC}

% Here we cover 5 methods of pre-processing variable length sequences for TSC. An overview of all the methods is described in the image below.

% \import{images/}{pre_processing.tex}
% \import{images/}{pre_processing_results.tex}

% Given our unbalanced dataset, there is also a need to deal with the unbalanced classes. 

% \subsubsection{Truncation / Padding}
% The most naive method to transform sequences to equal length is to either truncate or pad the sequence with dummy values. Truncation is performed by discarding the beginning or the end of the sequence to a shorter desired length. On the other hand, padding will add a dummy values to the beginning or the end of the sequence to a new longer length. The two methods can be used jointly as well. For example, say there are variable length sequences with a lengths that vary from $N$ to $M$ and we want a fixed length of $L$. We can truncate sequences with a length larger $L$ or more and pad sequences with a length less than $L$.

% \subsubsection{Resampling through interpolation / Extrapolation}
% Firstly, the difference between interpolation and extrapolation should be clarified. Interpolation is to fill gaps in between the given data points, whereas extrapolation is to predict future values from the given data points. 
% Although there are many methods to perform interpolation and extrapolation, the physical bounds that arise from nature of the data used in this study makes the Kalman filter a fitting method. 

% \subsubsection{Randomized Time Warping}
% Randomised time warping(RTW) is a randomised extension of dynamic time warping(DTW) for motion recognition\cite{suryanto2016randomized}. RTW generates time elastic (TE) features by randomly sampling the sequential data while retaining the temporal information. A set of TE features is represented by a low-dimensional subspace, called the sequence hypothesis (Hypo) subspace, and the similarity between two sequential patterns is defined by the canonical angles between the two corresponding Hypo subspaces. In essence, RTW simultaneously computes multiple degrees of similarities between a number of warped patterns’ pair candidates, while in practice, RTW generalizes the Hankel matrix commonly used in modeling of system dynamics. 

% % \subsubsection{Symbolic Aggregate Approximation}
% % Symbolic Aggregate approximation (SAX) is a classical symbolic approach for time series data mining, the basic concept of which is to convert the numerical form of a time series into a sequence of discrete symbols according to designated mapping rules. SAX can reduce dimensionality/ numerosity of data and has a lower bound to the Euclidean distance, that is, the error between the distance in the SAX representation and the Euclidean distance in the original data is bounded.

% \subsubsection{Under-Bagging}
% Diversity Exploration and Negative Correlation Learning on Imbalanced Data Sets.

% \subsection{Feature Extraction for TSC}

% \subsection{classification for TSC}
% One of the most popular and traditional TSC approaches is the use of a nearest neighbor (NN) classifier coupled with a distance function \cite{bagnall2017great}. Particularly, the Dynamic Time Warping (DTW) distance when used with a NN classifier has been shown to be a very strong baseline\cite{lines2015time}.

% \subsubsection{K-Nearest Neighbors + Dynamic Time Warping}

% \subsubsection{Hierarchical Vote Collective of Transformation-based Ensembles}

% \subsubsection{Time Warped ARMA models}

% \subsubsection{Enhanced Grassmann Discriminant Analysis with Randomized Time Warping}

% \subsubsection{Graph Convolutional Networks}

% An Attention Enhanced Graph Convolutional LSTM Network for Skeleton-Based Action Recognition
% Spatial Temporal Graph Convolutional Networks for Skeleton-Based Action Recognition

% \subsection{Subspace Based Methods}


% \section{Actionable Intelligence}